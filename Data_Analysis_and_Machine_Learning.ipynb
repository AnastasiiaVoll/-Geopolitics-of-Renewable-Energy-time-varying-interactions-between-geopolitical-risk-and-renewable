{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnastasiiaVoll/-Geopolitics-of-Renewable-Energy-time-varying-interactions-between-geopolitical-risk-and-renewable/blob/main/Data_Analysis_and_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCaRQBbOqlYn"
      },
      "source": [
        "#Step 1: Uploading and Cleaning the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBjJSs0kC4PK"
      },
      "source": [
        "1.1. Uploading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPjTKZXLPAh5"
      },
      "outputs": [],
      "source": [
        "pip install fancyimpute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX9cCpAkfkC4"
      },
      "outputs": [],
      "source": [
        "pip install category_encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8hF8pnqQIJc"
      },
      "outputs": [],
      "source": [
        "pip install graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Knsl44eKnulF"
      },
      "outputs": [],
      "source": [
        "pip install scikeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGjoKlaL2Df3"
      },
      "outputs": [],
      "source": [
        "pip install keras sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biQfdLYQ46S5"
      },
      "outputs": [],
      "source": [
        "pip install researchpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipvvFOPiULJP"
      },
      "outputs": [],
      "source": [
        "pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQxV0SgO9nDY"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary libraries\n",
        "# General libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import glob\n",
        "import graphviz\n",
        "import pickle\n",
        "from scipy.stats import pointbiserialr, chi2_contingency\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "\n",
        "# Sklearn libraries for preprocessing and model selection\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
        "\n",
        "# Sklearn libraries for metrics\n",
        "from sklearn.metrics import (confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, auc, \n",
        "                             accuracy_score, precision_score, recall_score, f1_score, classification_report)\n",
        "\n",
        "# Sklearn libraries for models\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.impute import KNNImputer\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Keras libraries for neural networks\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# Imbalance libraries for oversampling\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "\n",
        "# Special import for category encoders\n",
        "import category_encoders as ce\n",
        "\n",
        "# G Drive mount \n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7nPStsx-h4C"
      },
      "outputs": [],
      "source": [
        "# Connected to my Googel drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTR6nHB3-qDx"
      },
      "outputs": [],
      "source": [
        "# Specify the path to your files\n",
        "path = '/content/drive/MyDrive/ML& Analysis data'  # replace with your path\n",
        "\n",
        "# Create a list of all csv file names\n",
        "all_filenames = [i for i in glob.glob(path + '/*.csv')]\n",
        "\n",
        "# Read and concatenate datasets\n",
        "data = pd.concat([pd.read_csv(f) for f in all_filenames])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSScjZ9kATko"
      },
      "outputs": [],
      "source": [
        "# Sample 5% of my data\n",
        "data = data.sample(frac=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSTHqTX1J3Fo"
      },
      "outputs": [],
      "source": [
        "# A couple of vars I will not be using in any case, such as the record number of \n",
        "# an individul (as it is pseudo panel, so IDs are non repeating), etc., so I am droping them\n",
        "# Additioanlly, as meantioned in the report, I restrict the dataset variable range only to those variables that are readily avaliable on the close to real-time basis for the givernments on average\n",
        "vars_to_drop = ['REC_NUM', 'CMA', 'AGE_6', 'MJH', 'FTPTLAST', 'COWMAIN', 'NAICS_21', 'NOC_10', 'NOC_40', 'YABSENT', \n",
        "                'WKSAWAY', 'PAYAWAY', 'UHRSMAIN', 'AHRSMAIN', 'FTPTMAIN', 'UTOTHRS', 'ATOTHRS', 'HRSAWAY', 'YAWAY', \n",
        "                'PAIDOT', 'UNPAIDOT', 'XTRAHRS', 'WHYPT', 'TENURE', 'PREVTEN', 'HRLYEARN', 'PERMTEMP', 'ESTSIZE', \n",
        "                'FIRMSIZE', 'AVAILABL', 'LKPUBAG', 'LKEMPLOY', 'LKRELS', 'LKATADS', 'LKANSADS', 'LKOTHERN', 'YNOLOOK', \n",
        "                'TLOLOOK', 'EFAMTYPE', 'FINALWT']\n",
        "\n",
        "data = data.drop(vars_to_drop, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg7QiiQ5EBSH"
      },
      "outputs": [],
      "source": [
        "# LFSSTAT, the primary var of interest, is the Labour force status, categorical var \n",
        "# that has values:\n",
        "# 1\t= Employed (at work),  \n",
        "# 2\t= Employed (absent from work), \n",
        "# 3 =\tUnemployed, 4 =\tNot in labour force\n",
        "# I am interested in predicting the employed status, so I modify it into a binary var\n",
        "\n",
        "# Function to apply\n",
        "def modify_values(val):\n",
        "    if val == 1:\n",
        "        return 1\n",
        "    if val == 2:\n",
        "        return 1        \n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Applying this function to the LFSSTAT var\n",
        "data['LFSSTAT'] = data['LFSSTAT'].apply(modify_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXrx9hvqDsL7"
      },
      "outputs": [],
      "source": [
        "# First 5 rows of the data to eyebol \n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sz6bPk173R5p"
      },
      "outputs": [],
      "source": [
        "# Bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "data['LFSSTAT'].value_counts().plot(kind='bar', color=['skyblue', 'navy'])\n",
        "plt.title('Distribution of Employment Status')\n",
        "plt.xticks([0, 1], ['Employed', 'Otherwise'], rotation=0)\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo0DOR6a3XRU"
      },
      "outputs": [],
      "source": [
        "# Pie chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "data['LFSSTAT'].value_counts().plot(kind='pie', autopct='%1.1f%%', labels=['Employed', 'Otherwise'], colors=['skyblue', 'navy'])\n",
        "plt.title('Proportion of Employment Status')\n",
        "plt.ylabel('') \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhrdeTPnRKCc"
      },
      "source": [
        "1.2. Dealing with Missing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUyK-WEFftFh"
      },
      "outputs": [],
      "source": [
        "# Unfortunately, a number of the models that I plan to utilize are not capable of\n",
        "# handling missing data. Most of them really. As a result, it's crucial to completely handle missing \n",
        "# data before proceeding. \n",
        "\n",
        "# Checking for missing values\n",
        "print(\"Missing values:\")\n",
        "print(data.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO4nfIBjDax7"
      },
      "outputs": [],
      "source": [
        "# Checking for missing values, percentage wise \n",
        "print(\"Missing values in percentage:\")\n",
        "missing_percentage = (data.isnull().sum() / len(data)) * 100\n",
        "print(missing_percentage)\n",
        "\n",
        "# As a rough guideline, if a variable has more than 50% to 70% missing values, \n",
        "# I consider dropping the variable since the majority of the data is missing. \n",
        "# Imputing such a large proportion might lead to highly biased results.\n",
        "\n",
        "# I decided on a 75% threshold, which is somewhat arbitrary, of course. A better \n",
        "# approach would indeed be to try out different thresholds together with multiple \n",
        "# ways of imputing the missing data. However, the amount of computational power \n",
        "# and time needed for that given the immence size of my dataset goes beyond this assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKO-a3PWFw4p"
      },
      "outputs": [],
      "source": [
        "# Threshold\n",
        "threshold = 0.75\n",
        "\n",
        "# Identifying columns to drop\n",
        "cols_to_drop = data.columns[data.isnull().mean() > threshold]\n",
        "\n",
        "# Droping identified columns\n",
        "data = data.drop(cols_to_drop, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdE1y2wIIGfh"
      },
      "outputs": [],
      "source": [
        "# Checking for missing values, percentage wise \n",
        "print(\"Missing values in percentage:\")\n",
        "missing_percentage = (data.isnull().sum() / len(data)) * 100\n",
        "print(missing_percentage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GbAX5S1IZFt"
      },
      "outputs": [],
      "source": [
        "# I've managed to significantly reduce the missing data in the dataset, although \n",
        "# at the expense of discarding some potentially valuable information. \n",
        "\n",
        "# At this point, I need to determine how to manage rows (individuals) that still \n",
        "# have missing data. There are primarily two strategies I can consider:\n",
        "# 1. Deleting: I can eliminate rows that have any missing data. The advantage of \n",
        "# this method is its simplicity and the fact that it doesn't introduce additional \n",
        "# bias through assumptions. However, the downside is that I might lose important \n",
        "# information, especially if the missing data is not completely at random. If the \n",
        "# data is missing in a systematic way, eliminating these rows could introduce bias.\n",
        "# 2. Imputing: Alternatively, I can impute missing data, which means I estimate \n",
        "# and fill in the missing values based on the information I have. There are several\n",
        "# methods to do this, such as mean/median imputation, regression imputation, or \n",
        "# using machine learning models like k-Nearest Neighbors or multiple imputations\n",
        "# by chained equations (MICE). The advantage is that I keep the data and avoid \n",
        "# losing information. However, the downside is that I introduce some level of \n",
        "# assumption into the dataset, which might lead to misrepresentation if my imputation\n",
        "# strategy does not reflect the true nature of the data.\n",
        "\n",
        "# Before deciding on my steps, I look into the state of missing values in the data\n",
        "missing_rows = data.isnull().any(axis=1).sum()\n",
        "total_rows = data.shape[0]\n",
        "percentage_missing = (missing_rows / total_rows) * 100\n",
        "\n",
        "print(f\"Number of rows with at least one missing value: {missing_rows}\")\n",
        "print(f\"Percentage of rows with at least one missing value: {percentage_missing}%\")\n",
        "\n",
        "for i in [3, 5, 10, 15, 20, 25]:\n",
        "    missing_rows_i = data[data.isnull().sum(axis=1) > i].shape[0]\n",
        "    percentage_missing_i = (missing_rows_i / total_rows) * 100\n",
        "    print(f\"Number of rows with more than {i} missing values: {missing_rows_i}\")\n",
        "    print(f\"Percentage of rows with more than {i} missing values: {percentage_missing_i}%\")\n",
        "\n",
        "# Given that I have approximately 100% of individuals in the dataset with at least\n",
        "# one missing value, simply deleting the rows with missing values would not be possible\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMkjBxBOnpTq"
      },
      "outputs": [],
      "source": [
        "# While writing code for imputing, I will need to take into account a type of the data I am dealing with. \n",
        "#Checking the data types\n",
        "print(data.dtypes)\n",
        "\n",
        "# No all of them are correct, more should be int instead of float, but I cannot convert a var into\n",
        "# int if it has missing data. So first I need to deal with it. \n",
        "\n",
        "# I plant to deal with the missing data using several approaches: \n",
        "# Please add shortly beneforts and minisuses of all approaches ....\n",
        "# -> Mean / Most Freaquent missing data imputation (# Dataset 1, imputing using SimpleImputer)\n",
        "# -> MICE missing data imputation (# Dataset 2, imputing using IterativeImputer (MICE))\n",
        "# -> KNN missing data imputation (# Dataset 3, imputing using KNNImputer (KNN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZkyopmWE3fC"
      },
      "outputs": [],
      "source": [
        "# Function for converting data types\n",
        "def convert_data_types(df, columns_int, columns_float):\n",
        "    # Convert each column to integer and float data types respectively\n",
        "    for column in columns_int:\n",
        "        df[column] = df[column].astype(int)\n",
        "    for column in columns_float:\n",
        "        df[column] = df[column].astype(float)\n",
        "    return df\n",
        "\n",
        "columns_to_convert_int = ['SURVMNTH', 'LFSSTAT', 'PROV', 'AGE_12', 'SEX', 'MARSTAT', 'EDUC',  'EVERWORK', 'IMMIG', 'UNION', 'SCHOOLN', 'AGYOWNK']\n",
        "columns_to_convert_float = ['DURJLESS']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTC-J39oFSV0"
      },
      "outputs": [],
      "source": [
        "# Dataset 1, imputing using SimpleImputer\n",
        "data_mean = data.copy()\n",
        "\n",
        "# Here I do need to take into account the different types of data\n",
        "# Why? As the cmissing ontinous vars will be imputed using the mean, but that would be\n",
        "# a wrong approach to apply to the cathegorical vars. For the cathegorical vars I \n",
        "# impute them using most frequent value \n",
        "# Listing of columns to be imputed with mean\n",
        "mean_columns = ['DURJLESS']\n",
        "# Listting of columns to be imputed with most frequent value\n",
        "freq_columns = ['SURVMNTH', 'LFSSTAT', 'PROV', 'AGE_12', 'SEX', 'MARSTAT', 'EDUC',  'EVERWORK', 'IMMIG', 'UNION', 'SCHOOLN', 'AGYOWNK']\n",
        "\n",
        "# Imputation\n",
        "imputer_mean = SimpleImputer(strategy='mean')\n",
        "imputer_freq = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "data_mean[mean_columns] = imputer_mean.fit_transform(data_mean[mean_columns])\n",
        "data_mean[freq_columns] = imputer_freq.fit_transform(data_mean[freq_columns])\n",
        "\n",
        "# Mean/Median/Mode Imputation: This is a simple technique where the missing values\n",
        "# of a certain variable are filled in with its mean, median, or mode. While this \n",
        "# method is very easy to implement, it has the downside of potentially leading to \n",
        "# biased estimates, especially if the data isn't missing completely at random.\n",
        "\n",
        "# Concatenating the year and month variables into a string, then convert to datetime\n",
        "data_mean['Date'] = pd.to_datetime(data_mean['SURVYEAR'].astype(str) + '-' + data_mean['SURVMNTH'].astype(int).astype(str), format='%Y-%m')\n",
        "data_mean = data_mean.drop(columns=['SURVYEAR']) # Survey year\n",
        "\n",
        "# Converting each column to integer or float data type\n",
        "data_mean = convert_data_types(data_mean, columns_to_convert_int, columns_to_convert_float)\n",
        "print(data_mean.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v0GamGXAUdL"
      },
      "outputs": [],
      "source": [
        "pickle.dump(data_mean, open('/content/drive/MyDrive/ML& Analysis data/data_mean.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxebgW6RAnIW"
      },
      "outputs": [],
      "source": [
        "# data_mean = pickle.load(open('/content/drive/MyDrive/ML& Analysis data/data_mean.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKzdD-VGP0W1"
      },
      "outputs": [],
      "source": [
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOStVCdt-HEW"
      },
      "outputs": [],
      "source": [
        "print(data_mean.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKLHhN43RC51"
      },
      "outputs": [],
      "source": [
        "# Dataset 2, imputing using IterativeImputer (MICE)\n",
        "data_mice = data.copy()\n",
        "\n",
        "def perform_imputation(data, categorical_columns):\n",
        "    \"\"\"\n",
        "    Imputes missing values using IterativeImputer.\n",
        "\n",
        "    Parameters:\n",
        "    - data: Dataframe with missing values\n",
        "    - categorical_columns: List of categorical columns in the dataframe\n",
        "\n",
        "    Returns:\n",
        "    - Dataframe with imputed values\n",
        "    \"\"\"\n",
        "    # Initializing an imputer\n",
        "    imputer = IterativeImputer()\n",
        "\n",
        "    # Performing imputation\n",
        "    data_imputed = imputer.fit_transform(data)\n",
        "\n",
        "    # Creating a new dataframe with the imputed data and original column names\n",
        "    data_imputed = pd.DataFrame(data_imputed, columns=data.columns)\n",
        "\n",
        "    # Rounding and cast to integers for categorical variables\n",
        "    for col in categorical_columns:\n",
        "        data_imputed[col] = np.round(data_imputed[col]).astype(int)\n",
        "\n",
        "    return data_imputed\n",
        "\n",
        "def convert_data_types(data_mice, columns_to_convert_int, columns_to_convert_float):\n",
        "    \"\"\"\n",
        "    Converts specific columns in the dataframe to the desired data types.\n",
        "\n",
        "    Parameters:\n",
        "    - data_mice: The dataframe\n",
        "    - columns_to_convert_int: List of columns to convert to integer data type\n",
        "    - columns_to_convert_float: List of columns to convert to float data type\n",
        "\n",
        "    Returns:\n",
        "    - Dataframe with converted data types\n",
        "    \"\"\"\n",
        "    # Converting each column to integer data type\n",
        "    for column in columns_to_convert_int:\n",
        "        data_mice[column] = data_mice[column].astype(int)\n",
        "\n",
        "    # Converting each column to float data type\n",
        "    for column in columns_to_convert_float:\n",
        "        data_mice[column] = data_mice[column].astype(float)\n",
        "        \n",
        "    return data_mice\n",
        "\n",
        "# Categorical columns\n",
        "categorical_columns = ['SURVMNTH', 'LFSSTAT', 'PROV', 'AGE_12', 'SEX', 'MARSTAT', 'EDUC',  'EVERWORK', 'IMMIG', 'UNION', 'SCHOOLN', 'AGYOWNK']\n",
        "\n",
        "# Performing imputation\n",
        "data_mice = perform_imputation(data_mice, categorical_columns)\n",
        "\n",
        "# Converting SURVYEAR to integer, then concatenate with SURVMNTH to create a Date column\n",
        "data_mice['Date'] = pd.to_datetime(data_mice['SURVYEAR'].astype(int).astype(str) + '-' + data_mice['SURVMNTH'].astype(int).astype(str), format='%Y-%m')\n",
        "data_mice = data_mice.drop(columns=['SURVYEAR']) # Survey year\n",
        "\n",
        "# Converting data types\n",
        "data_mice = convert_data_types(data_mice, columns_to_convert_int, columns_to_convert_float)\n",
        "\n",
        "# Check \n",
        "print(data_mice.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi92mE4vQbrR"
      },
      "outputs": [],
      "source": [
        "# MICE imputed data summary\n",
        "print(\"\\nMICE Imputed Data:\")\n",
        "print(data_mice.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTy4By2aP453"
      },
      "outputs": [],
      "source": [
        "print(data.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXj0PYqQP4r5"
      },
      "outputs": [],
      "source": [
        "print(data_mean.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dygAtpgvcQd4"
      },
      "outputs": [],
      "source": [
        "# Note the problem with the MICE imputer: \n",
        "# The IterativeImputer from sklearn uses a machine learning model (default is BayesianRidge) to predict missing values based on other variables. It does not inherently know that certain variables are categorical and should be within certain limits. As a result, it can sometimes generate predictions that do not fall into the expected categories.\n",
        "# So I need to turn to a diffrent option. KNNImputer, inherently respect the bounds of the data because they base their imputations on observed values in the dataset.\n",
        "# Before running the KNNImputer function, I need to normalize the data. Normalization is often a good practice before using distance-based algorithms like K-Nearest Neighbors (KNN). This is because KNN calculates the distance between pairs of data points to find the nearest neighbors; if one variable has a very large scale compared to others, it may dominate the distance calculations, leading to biased results.\n",
        "# NOTE: this will take a long time to run. Preferdably do not repeat. I needed to leave the laptop working over night. \n",
        "\n",
        "# Dataset 3, imputing using KNNImputer (KNN)\n",
        "# Create a mask for missing values\n",
        "missing_mask = data.isnull().any(axis=1)\n",
        "\n",
        "# Separate complete rows and rows with missing values\n",
        "data_complete = data.loc[~missing_mask]\n",
        "data_missing = data.loc[missing_mask]\n",
        "\n",
        "# List of continuous and categorical columns\n",
        "continuous_columns = ['DURJLESS']\n",
        "categorical_columns = ['SURVYEAR', 'SURVMNTH', 'LFSSTAT', 'PROV', 'AGE_12', 'SEX', 'MARSTAT', 'EDUC', 'EVERWORK', 'IMMIG', 'UNION', 'SCHOOLN', 'AGYOWNK']\n",
        "\n",
        "# Call the function to impute missing values using KNN\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "data_missing_imputed = pd.DataFrame(imputer.fit_transform(data_missing), columns=data_missing.columns)\n",
        "\n",
        "# Concatenate the complete rows and imputed rows\n",
        "data_knn = pd.concat([data_complete, data_missing_imputed])\n",
        "\n",
        "# Convert SURVYEAR to integer, then concatenate with SURVMNTH to create a Date column\n",
        "data_knn['Date'] = pd.to_datetime(data_knn['SURVYEAR'].astype(int).astype(str) + '-' + data_knn['SURVMNTH'].astype(int).astype(str), format='%Y-%m')\n",
        "data_knn = data_knn.drop(columns=['SURVYEAR']) # Survey year\n",
        "\n",
        "# Convert data types\n",
        "data_knn = convert_data_types(data_knn, columns_to_convert_int, columns_to_convert_float)\n",
        "\n",
        "# Print the data types of the dataframe to confirm the changes\n",
        "print(data_knn.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8O-y_QbA4YO"
      },
      "outputs": [],
      "source": [
        "# Saving the dataset \n",
        "pickle.dump(data_knn, open('/content/drive/MyDrive/ML& Analysis data/data_knn.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeIruL7EtHJI"
      },
      "outputs": [],
      "source": [
        "# data_knn = pickle.load(open('/content/drive/MyDrive/ML& Analysis data/data_knn.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpWkbTWlRpA5"
      },
      "outputs": [],
      "source": [
        "# Now I summarize and set up a dictionary for the datasets created for the later steps\n",
        "def print_summary_statistics(data_dict):\n",
        "    \"\"\"\n",
        "    Print summary statistics for each DataFrame in the input dictionary.\n",
        "    \n",
        "    Parameters:\n",
        "    - data_dict: a dictionary where keys are descriptions and values are DataFrames\n",
        "    \n",
        "    \"\"\"\n",
        "    for description, df in data_dict.items():\n",
        "        print(f\"\\n{description} Data:\")\n",
        "        print(df.describe())\n",
        "\n",
        "# Create a dictionary to hold the datasets\n",
        "data_dict = {\n",
        "    \"Original\": data,\n",
        "    \"Mean Imputed\": data_mean,\n",
        "    \"MICE Imputed\": data_mice,\n",
        "    \"KNN Imputed\": data_knn\n",
        "}\n",
        "\n",
        "# Print summary statistics for each dataset\n",
        "print_summary_statistics(data_dict)\n",
        "\n",
        "# KNN seems to perofrm quite reasonbley, I remove the MICE imputed data due to the previously mentioned problem with imputing the cathegorical vars, and will be using the KNN imputed data insted.\n",
        "# Create a dictionary to hold the datasets\n",
        "data_dict = {\n",
        "    \"Mean Imputed\": data_mean,\n",
        "    \"KNN Imputed\": data_knn\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShVaOWkQtwCV"
      },
      "outputs": [],
      "source": [
        "data_dict = {\n",
        "    \"Mean Imputed\": data_mean,\n",
        "    \"KNN Imputed\": data_knn\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lGF0BuVb2kO"
      },
      "outputs": [],
      "source": [
        "# Final check of missing values before procceding further\n",
        "for dataset_name, dataset in data_dict.items():\n",
        "    # Calculate missing values in percentage\n",
        "    missing_percentage = (dataset.isnull().sum() / len(dataset)) * 100\n",
        "    # Print the missing values percentage for each dataset\n",
        "    print(f\"\\n{dataset_name} Data: Missing values in percentage\")\n",
        "    print(missing_percentage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blcAek7DkeV8"
      },
      "outputs": [],
      "source": [
        "# Checking the datasets details \n",
        "for dataset_name, dataset in data_dict.items():\n",
        "    dataset.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVqq1P0gfbqB"
      },
      "source": [
        "1.3. Merging the macro-economic data with the LFS datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh9IEXRCEmVi"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/ML& Analysis data/Canada_macro_data.xlsx' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFcoJF9w8ZG0"
      },
      "outputs": [],
      "source": [
        "# Load the Excel file\n",
        "economic_indicators = pd.read_excel(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3KNy0FwIpoM"
      },
      "outputs": [],
      "source": [
        "print(\"Columns in economic_indicators: \", economic_indicators.columns)\n",
        "\n",
        "for name, data in data_dict.items():\n",
        "    print(f\"Columns in {name}: \", data.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQL-1MvqI-HL"
      },
      "outputs": [],
      "source": [
        "# Confirm data type of 'Date' column\n",
        "print(\"Data type of 'Date' in economic_indicators: \", economic_indicators['Date'].dtype)\n",
        "\n",
        "for name, data in data_dict.items():\n",
        "    print(f\"Data type of 'Date' in {name}: \", data['Date'].dtype)\n",
        "    \n",
        "# Check uniqueness of 'Date' column\n",
        "print(\"Is 'Date' unique in economic_indicators: \", economic_indicators['Date'].is_unique)\n",
        "\n",
        "for name, data in data_dict.items():\n",
        "    print(f\"Is 'Date' unique in {name}: \", data['Date'].is_unique)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrwp03v6IGid"
      },
      "outputs": [],
      "source": [
        "# Convert the 'Date' column to datetime format in 'economic_indicators' DataFrame\n",
        "economic_indicators['Date'] = pd.to_datetime(economic_indicators['Date'])\n",
        "\n",
        "# Make sure the 'Date' column in 'data_knn' DataFrame is also in datetime format\n",
        "data_knn['Date'] = pd.to_datetime(data_knn['Date'])\n",
        "\n",
        "# Merge the dataframes\n",
        "data_knn = pd.merge(data_knn, economic_indicators, on='Date', how='left')\n",
        "\n",
        "# Make sure the 'Date' column in 'data_knn' DataFrame is also in datetime format\n",
        "data_mean['Date'] = pd.to_datetime(data_mean['Date'])\n",
        "\n",
        "# Merge the dataframes\n",
        "data_mean = pd.merge(data_mean, economic_indicators, on='Date', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjdsCVhjKtjc"
      },
      "outputs": [],
      "source": [
        "# Print the data types of the dataframe to confirm the changes\n",
        "print(data_mean.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlI4GeUG5Ze8"
      },
      "outputs": [],
      "source": [
        "# Describe dataset 0 \n",
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZrhXpJjTaD6"
      },
      "outputs": [],
      "source": [
        "# Describe dataset 1\n",
        "data_mean.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91G__ekcmYzJ"
      },
      "outputs": [],
      "source": [
        "# Describe dataset 3\n",
        "# # Describe dataset 3 I do not use fo rthe resons mentioned before in the code \n",
        "data_knn.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AeitHrXwT6A"
      },
      "outputs": [],
      "source": [
        "# I want to see, before proceeding, which vars correlate the most with my var of interest LFSSTAT\n",
        "# I take account for the fact that some vars I have are continious and others are cathegoriecal\n",
        "\n",
        "# Listing the variables by their type\n",
        "categorical  = [ 'SURVMNTH', 'LFSSTAT', 'PROV', 'AGE_12', 'SEX', 'MARSTAT', 'EDUC',  'EVERWORK', 'IMMIG', 'UNION', 'SCHOOLN', 'AGYOWNK']\n",
        "continuous = [\n",
        "    'DURJLESS', \n",
        "    'Growth rate same period previous year, s.a. Percentage',\n",
        "    'Level, rate or national currency, s.a. Ratio',\n",
        "    'Level, rate or national currency, s.a. Canadian Dollar, Millions'\n",
        "]\n",
        "\n",
        "\n",
        "# Dictionary to store correlations\n",
        "correlations = {}\n",
        "\n",
        "# Calculating correlations\n",
        "for var in categorical + continuous:\n",
        "    if var != 'LFSSTAT':\n",
        "        if var in categorical:\n",
        "            # For binary-categorical pairs, compute Cramer's V\n",
        "            contingency_table = pd.crosstab(data_knn['LFSSTAT'], data_knn[var])\n",
        "            chi2, _, _, _ = chi2_contingency(contingency_table)\n",
        "            n = contingency_table.sum().sum()\n",
        "            phi2 = chi2/n\n",
        "            r,k = contingency_table.shape\n",
        "            phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
        "            rcorr = r-((r-1)**2)/(n-1)\n",
        "            kcorr = k-((k-1)**2)/(n-1)\n",
        "            correlations[var] = np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n",
        "        else:\n",
        "            # For binary-continuous pairs, compute point biserial correlation coefficient\n",
        "            correlations[var] = pointbiserialr(data_knn['LFSSTAT'], data_knn[var])[0]\n",
        "\n",
        "\n",
        "# Sorting by absolute value\n",
        "sorted_corr = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "# Selecting the top 6 (a bit arbitraty, but should give the main idea)\n",
        "top_6_corr = dict(sorted_corr[:6])\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(pd.DataFrame(top_6_corr, index=['LFSSTAT']), annot=True, cmap='Blues')\n",
        "plt.title('Top 6 Variables Most Correlated with LFSSTAT')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0E9h0zLePPM"
      },
      "outputs": [],
      "source": [
        "# Creating a subplot\n",
        "fig, axs = plt.subplots(3, 2, figsize=(15, 20))\n",
        "\n",
        "# Reshape axs to 1D\n",
        "axs = axs.ravel()\n",
        "\n",
        "# Generate boxplots\n",
        "for i, var in enumerate(top_6_corr.keys()):\n",
        "    sns.boxplot(x='LFSSTAT', y=var, data=data, palette='Blues', ax=axs[i])\n",
        "    axs[i].set_title(f'Boxplot of {var} by LFSSTAT')\n",
        "\n",
        "# Delete any unused subplots\n",
        "if len(top_6_corr.keys()) % 2 != 0:\n",
        "    fig.delaxes(axs[-1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTvbXriEgho0"
      },
      "outputs": [],
      "source": [
        "# I want to see, before proceeding, which vars correlate the most with my var of interest LFSSTAT\n",
        "# I take account for the fact that some vars I have are continious and others are cathegoriecal\n",
        "\n",
        "# Listing the variables by their type\n",
        "categorical  = [ 'SURVMNTH', 'LFSSTAT', 'PROV', 'AGE_12', 'SEX', 'MARSTAT', 'EDUC',  'EVERWORK', 'IMMIG', 'UNION', 'SCHOOLN', 'AGYOWNK']\n",
        "continuous = [\n",
        "    'DURJLESS', \n",
        "    'Growth rate same period previous year, s.a. Percentage',\n",
        "    'Level, rate or national currency, s.a. Ratio',\n",
        "    'Level, rate or national currency, s.a. Canadian Dollar, Millions'\n",
        "]\n",
        "\n",
        "\n",
        "# Dictionary to store correlations\n",
        "correlations = {}\n",
        "\n",
        "# Calculating correlations\n",
        "for var in categorical + continuous:\n",
        "    if var != 'LFSSTAT':\n",
        "        if var in categorical:\n",
        "            # For binary-categorical pairs, compute Cramer's V\n",
        "            contingency_table = pd.crosstab(data_mean['LFSSTAT'], data_mean[var])\n",
        "            chi2, _, _, _ = chi2_contingency(contingency_table)\n",
        "            n = contingency_table.sum().sum()\n",
        "            phi2 = chi2/n\n",
        "            r,k = contingency_table.shape\n",
        "            phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
        "            rcorr = r-((r-1)**2)/(n-1)\n",
        "            kcorr = k-((k-1)**2)/(n-1)\n",
        "            correlations[var] = np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n",
        "        else:\n",
        "            # For binary-continuous pairs, compute point biserial correlation coefficient\n",
        "            correlations[var] = pointbiserialr(data_mean['LFSSTAT'], data_mean[var])[0]\n",
        "\n",
        "\n",
        "# Sorting by absolute value\n",
        "sorted_corr = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "# Selecting the top 6 (a bit arbitraty, but should give the main idea)\n",
        "top_6_corr = dict(sorted_corr[:6])\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(pd.DataFrame(top_6_corr, index=['LFSSTAT']), annot=True, cmap='Blues')\n",
        "plt.title('Top 6 Variables Most Correlated with LFSSTAT')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVtq7SyigkNH"
      },
      "outputs": [],
      "source": [
        "# Creating a subplot\n",
        "fig, axs = plt.subplots(3, 2, figsize=(15, 20))\n",
        "\n",
        "# Reshape axs to 1D\n",
        "axs = axs.ravel()\n",
        "\n",
        "# Generate boxplots\n",
        "for i, var in enumerate(top_6_corr.keys()):\n",
        "    sns.boxplot(x='LFSSTAT', y=var, data=data_mean, palette='Blues', ax=axs[i])\n",
        "    axs[i].set_title(f'Boxplot of {var} by LFSSTAT')\n",
        "\n",
        "# Delete any unused subplots\n",
        "if len(top_6_corr.keys()) % 2 != 0:\n",
        "    fig.delaxes(axs[-1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8qex5mihu1I"
      },
      "source": [
        "1.4. Types of data, hot ecoding and etc. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tbf5Koy_y0WC"
      },
      "outputs": [],
      "source": [
        "# Checking the cathegorical vars, and wheather thay have been imputed correctly and the format and stucture of them is still intact \n",
        "def print_unique_counts(data_dict, columns):\n",
        "    \"\"\"\n",
        "    Counts and prints the unique values in specified columns of a DataFrame.\n",
        "    \n",
        "    Parameters:\n",
        "    - data_dict: dictionary of pandas DataFrames\n",
        "    - columns: list of column names to count unique values in\n",
        "    \"\"\"\n",
        "    for data_name, df in data_dict.items():\n",
        "        print(f\"Dataset: {data_name}\")\n",
        "        for column in columns:\n",
        "            counts = df[column].value_counts()\n",
        "            print(f\"Unique value counts in '{column}':\\n{counts}\\n\")\n",
        "\n",
        "# Listing of columns to count unique values in\n",
        "columns_to_count = [ 'SURVMNTH', 'LFSSTAT', 'PROV', 'AGE_12', 'SEX', 'MARSTAT', 'EDUC',  'EVERWORK', 'IMMIG', 'UNION', 'SCHOOLN', 'AGYOWNK']\n",
        "\n",
        "# Calling the function\n",
        "print_unique_counts(data_dict, columns_to_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaKUmuonzQu-"
      },
      "outputs": [],
      "source": [
        "# Performing one-hot encoding for each dataset\n",
        "columns_to_encode = ['SURVMNTH', 'PROV', 'AGE_12', 'MARSTAT', 'EDUC', 'EVERWORK', 'IMMIG', 'AGYOWNK']\n",
        "\n",
        "for data_name in data_dict.keys():\n",
        "    data_dict[data_name] = pd.get_dummies(data_dict[data_name], columns=columns_to_encode)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWDtlFZZzmVj"
      },
      "outputs": [],
      "source": [
        "# Other vars I want to transform into binary, where I think I will not results in the loss of information \n",
        "# Define a function to recategorize variables\n",
        "def recategorize(val):\n",
        "    if val == 1 or val == 2:\n",
        "        return val\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "# Define the columns to recategorize\n",
        "columns_to_recategorize = ['UNION', 'SCHOOLN']\n",
        "\n",
        "# Apply the function to each dataset in the dictionary for each specified column\n",
        "for data_name in data_dict.keys():\n",
        "    for column in columns_to_recategorize:\n",
        "        data_dict[data_name][column] = data_dict[data_name][column].apply(recategorize)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_KNNqHsOI5p"
      },
      "outputs": [],
      "source": [
        "# Transforming the dependant var inot teh correct form\n",
        "for name, dataset in data_dict.items():\n",
        "    dataset['LFSSTAT'] = dataset['LFSSTAT'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqAyQHPPP4cY"
      },
      "source": [
        "1.3. Balancing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjQhqcH7Ddin"
      },
      "outputs": [],
      "source": [
        "# I am checking how ballanced is the depandant var LFSSTAT\n",
        "for name, dataset in data_dict.items():\n",
        "    print(f\"Dataset: {name}\")\n",
        "    print(dataset['LFSSTAT'].value_counts(normalize=True))  # Normalize=True to get proportions instead of count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjscInuG4HjM"
      },
      "outputs": [],
      "source": [
        "# Plotting the balance of the depandant var LFSSTAT for better overviw (+ to add later inot the report)\n",
        "# Set the Seaborn theme\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Use palette for blue colors\n",
        "blue_palette = sns.color_palette(\"Blues_r\")\n",
        "\n",
        "for name, dataset in data_dict.items():\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    \n",
        "    # Use seaborn countplot to get blue colors and bar widths that represent counts\n",
        "    sns.countplot(x=\"LFSSTAT\", data=dataset, palette=blue_palette)\n",
        "    \n",
        "    plt.title(f'LFSSTAT distribution in {name} dataset', fontsize=18)\n",
        "    plt.ylabel('Count', fontsize=14)\n",
        "    plt.xlabel('LFSSTAT', fontsize=14)\n",
        "    \n",
        "    # Replacing 0 and 1 with 'Otherwise' and 'Employed' for x-axis\n",
        "    plt.xticks([0, 1], ['Otherwise', 'Employed'])\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqagAcYYOu_g"
      },
      "source": [
        "#Step 2: Building Up the Models + Traning + Evaluating "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3Qpers0OyKd"
      },
      "source": [
        "2.1. Models: \n",
        "(i) LogisticRegression, \n",
        "(ii) SVC, \n",
        "(iii) DecisionTreeClassifier, \n",
        "(iv) RandomForestClassifier, \n",
        "(v) KNeighborsClassifier, \n",
        "(vi) GaussianNB, \n",
        "(vii) GradientBoostingClassifier, \n",
        "(viii) XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXjThW8qNRxB"
      },
      "outputs": [],
      "source": [
        "# Creating a dictionary for the GridSearch parameters for each algorithm\n",
        "param_dict = {\n",
        "    LogisticRegression: {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l2', 'None'],\n",
        "        'solver': ['lbfgs', 'liblinear']},\n",
        "\n",
        "    DecisionTreeClassifier: {\n",
        "        'max_depth': [3, 5, 10, 20],\n",
        "        'min_samples_leaf': [2, 4, 6]},\n",
        "    RandomForestClassifier: {\n",
        "        'n_estimators': [10, 100, 200],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt']},\n",
        "    KNeighborsClassifier: {\n",
        "        'n_neighbors': list(range(1,30)),\n",
        "        'weights': ['uniform', 'distance']},\n",
        "    GaussianNB: {},\n",
        "    GradientBoostingClassifier: {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'learning_rate': [0.01, 0.1, 1],\n",
        "        'max_depth': [3, 5, 10]\n",
        "    },\n",
        "    XGBClassifier: {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'learning_rate': [0.01, 0.1, 1],\n",
        "        'max_depth': [3, 5, 10],\n",
        "        'use_label_encoder':[False]\n",
        "    },\n",
        "}\n",
        "\n",
        "# Models to train\n",
        "models = [\n",
        "    LogisticRegression,\n",
        "    DecisionTreeClassifier,\n",
        "    RandomForestClassifier,\n",
        "    KNeighborsClassifier,\n",
        "    GaussianNB,\n",
        "    GradientBoostingClassifier,\n",
        "    XGBClassifier,\n",
        "]\n",
        "\n",
        "# Models to calculate feature importances for\n",
        "importance_models = [\n",
        "    GradientBoostingClassifier,\n",
        "    XGBClassifier,\n",
        "    RandomForestClassifier,\n",
        "]\n",
        "\n",
        "# Initialiing oversampler and results\n",
        "oversample = RandomUnderSampler()\n",
        "results = {}\n",
        "\n",
        "# Splitting, normalizing and training on each dataset\n",
        "for name, data in data_dict.items():\n",
        "    print(f\"\\nProcessing {name} dataset.\")\n",
        "    \n",
        "    # Initializing dataset results\n",
        "    results[name] = {}\n",
        "\n",
        "    # Separating features and target\n",
        "    X = data.drop(['LFSSTAT', 'Date'], axis=1)  # Remove 'Date' from feature set\n",
        "    y = data['LFSSTAT']\n",
        "\n",
        "    # Creating masks for the split\n",
        "    train_mask =  (data['Date'].dt.year < 2020)\n",
        "    test_mask = data['Date'].dt.year >= 2020\n",
        "\n",
        "    # Splitting based on masks\n",
        "    X_train, y_train = X[train_mask], y[train_mask]\n",
        "    X_test, y_test = X[test_mask], y[test_mask]\n",
        "    \n",
        "    # Balancing\n",
        "    X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "    \n",
        "    # Normalizing\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "    \n",
        "    # Model training\n",
        "for model in models:\n",
        "    if model.__name__ not in results[name]:  # if the model's results aren't in the dictionary\n",
        "        print(f\"\\nTraining {model.__name__}...\")\n",
        "        classifier = model()\n",
        "        grid_search = GridSearchCV(estimator=classifier, param_grid=param_dict[model], cv=5)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        \n",
        "        # Predicting\n",
        "        y_pred = grid_search.predict(X_test)\n",
        "        \n",
        "        # Store metrics\n",
        "        results[name][model.__name__] = {\n",
        "            'Best Params': grid_search.best_params_,\n",
        "            'Accuracy': accuracy_score(y_test, y_pred),\n",
        "            'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
        "            'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
        "            'F1-score': f1_score(y_test, y_pred, average='weighted'),\n",
        "            'AUC-ROC': roc_auc_score(y_test, y_pred)\n",
        "        }\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        plt.figure(figsize=(7,5))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title(f'Confusion Matrix for {model.__name__}')\n",
        "        plt.show()\n",
        "\n",
        "        # ROC curve\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.figure(figsize=(7,5))\n",
        "        plt.plot(fpr, tpr, color='darkblue', label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "        plt.plot([0, 1], [0, 1], color='lightblue', linestyle='--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'Receiver Operating Characteristic for {model.__name__}')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()\n",
        "\n",
        "        # Precision-Recall curve\n",
        "        precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
        "        auc_score = auc(recall, precision)\n",
        "        plt.figure(figsize=(7,5))\n",
        "        plt.plot(recall, precision, label=f'PRC curve (area = {auc_score:0.2f})', color='darkblue')\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.title(f'Precision-Recall Curve for {model.__name__}')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()\n",
        "\n",
        "        # Calculating feature importance if the model is in 'importance_models'\n",
        "        if model in importance_models:\n",
        "            print(f\"\\nCalculating feature importances for {model.__name__}...\")\n",
        "            best_model = grid_search.best_estimator_\n",
        "            importances = best_model.feature_importances_\n",
        "            feature_importances = pd.DataFrame({\n",
        "                'Feature': X.columns,\n",
        "                'Importance': importances,\n",
        "            }).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            sns.barplot(data=feature_importances[:10], x='Importance', y='Feature', palette='Blues_r')\n",
        "            plt.title(f'Top 10 Feature Importances for {model.__name__}')\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYP16E1uzvxn"
      },
      "outputs": [],
      "source": [
        "# See which hyperparameters got selected\n",
        "for dataset, model_metrics in results.items():\n",
        "    for model, metrics in model_metrics.items():\n",
        "        print(f\"Dataset: {dataset}, Model: {model}, Best Parameters: {metrics['Best Params']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4oO5iTRZcjP"
      },
      "outputs": [],
      "source": [
        "# See metrics for each model \n",
        "all_results_df = pd.DataFrame()\n",
        "\n",
        "for dataset_name, dataset_results in results.items():\n",
        "    for model_name, model_results in dataset_results.items():\n",
        "        model_results_df = pd.DataFrame(model_results, index=[0])\n",
        "        model_results_df.insert(0, 'Dataset', dataset_name)\n",
        "        model_results_df.insert(1, 'Model', model_name)\n",
        "        all_results_df = all_results_df.append(model_results_df, ignore_index=True)\n",
        "\n",
        "display(all_results_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzUiQOweMlyPULjdf16+2o",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}